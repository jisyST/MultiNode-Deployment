import os, sys
from multinode.multinode_deployment import MultiNodeDeployment
import torch
from fastapi import Request 
import requests
from dotenv import load_dotenv
load_dotenv() 

############################
# å¼ é‡å¹¶è¡Œ
############################

# è¿æ¥rayé›†ç¾¤å¹¶è·å–é›†ç¾¤çŠ¶æ€  
# ray_addressï¼š
# None -- å·²ç»å¯åŠ¨é›†ç¾¤åˆ™è¿æ¥é›†ç¾¤ï¼Œæœªå¯åŠ¨é›†ç¾¤åˆ™åœ¨æœ¬æœºå¯åŠ¨ä¸€ä¸ªå•æœºç‰ˆrayæœåŠ¡
# "auto" -- è¿æ¥å·²ç»å¯åŠ¨çš„é›†ç¾¤ï¼Œæœªå¯åŠ¨é›†ç¾¤æŠ¥é”™
# "ray://<head-node-ip>:10001" -- è¿æ¥è¿œç¨‹é›†ç¾¤
headnode_ip = os.getenv("RAY_HEAD_IP")
multinode_depoly = MultiNodeDeployment(f"ray://{headnode_ip}:10001")

# å®šä¹‰ä»»åŠ¡ç±» 
class MyDeployment:      
    def __init__(self):
        # åˆå§‹åŒ–æ—¶è®¾ç½®æ¨¡å‹å¹¶è¡Œï¼Œä½¿ç”¨2å¼ GPUï¼ˆtensor_parallel_size=2ï¼‰ã€‚
        # è¯·ç¡®ä¿åœ¨ Ray Serve éƒ¨ç½²æ—¶ï¼Œray_actor_options ä¸­é…ç½®äº† num_gpus=2ï¼Œ
        # è¿™æ · Ray ä¼šåœ¨åˆ†é…è¯¥è¿›ç¨‹æ—¶è®¾ç½® CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡ï¼Œåªæ˜¾ç¤º2å¼ GPUã€‚
        from vllm import LLM, SamplingParams
        cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "Not Set")
        print(f"[Init] CUDA_VISIBLE_DEVICES: {cuda_visible}")

        # æ‰“å°å½“å‰ GPU æƒ…å†µ
        print(f"[Init] torch.cuda.device_count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"[Init] GPU {i}: {torch.cuda.get_device_name(i)}")
            
        self.llm = LLM("/home/mnt/share_server/models/Qwen2-7B-Instruct", tensor_parallel_size=2)
        self.sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=512)

    async def __call__(self, request: Request):
        body = await request.json()
        prompt = body.get("prompt", "")

        outputs = self.llm.generate(prompt, self.sampling_params)
        return {"output": outputs[0].outputs[0].text}


# åˆå§‹åŒ–éƒ¨ç½²å¯¹è±¡   
multinode_depoly.initialize_deployment( 
    name='hello_world', 
    min_replicas=1,     # æœ€å°‘å‰¯æœ¬æ•° 
    max_replicas=1,     # æœ€å¤šå‰¯æœ¬æ•° 
    task_processor=MyDeployment,  # ç”¨æˆ·è‡ªå®šä¹‰ä»»åŠ¡API 
    num_gpus=2,
) 
    
# åœ¨é›†ç¾¤ç¯å¢ƒå¯åŠ¨éƒ¨ç½²å¯¹è±¡
multinode_depoly.run(port=8100) 


# ç­‰å¾…æœåŠ¡å¯åŠ¨ 
import time 
time.sleep(5) 


# å†æ¬¡æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
multinode_depoly.check_cluster_status()


# å•æ¬¡æ¨ç† 
API_URL = multinode_depoly.url

# æ„é€ è¯·æ±‚ä½“
data = {
    "prompt": "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ã€‚"
}

# å‘èµ· POST è¯·æ±‚
response = requests.post(API_URL, json=data)

# æ‰“å°è¿”å›ç»“æœ
if response.ok:
    print("ğŸ§  æ¨¡å‹å›ç­”ï¼š", response.json()["output"])
else:
    print("âŒ è¯·æ±‚å¤±è´¥ï¼š", response.status_code, response.text)

# å…³é—­æœåŠ¡
multinode_depoly.shut_down() 