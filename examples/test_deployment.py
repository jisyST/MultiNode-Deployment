import os, sys
from multinode.multinode_deployment import MultiNodeDeployment
from ray.runtime_env import RuntimeEnv
import torch
from ray import get_runtime_context 
import asyncio
from fastapi import Request 
from dotenv import load_dotenv
load_dotenv()  





# è¿æ¥rayé›†ç¾¤å¹¶è·å–é›†ç¾¤çŠ¶æ€  
# ray_addressï¼š
# None -- å·²ç»å¯åŠ¨é›†ç¾¤åˆ™è¿æ¥é›†ç¾¤ï¼Œæœªå¯åŠ¨é›†ç¾¤åˆ™åœ¨æœ¬æœºå¯åŠ¨ä¸€ä¸ªå•æœºç‰ˆrayæœåŠ¡
# "auto" -- è¿æ¥å·²ç»å¯åŠ¨çš„é›†ç¾¤ï¼Œæœªå¯åŠ¨é›†ç¾¤æŠ¥é”™
# "ray://<head-node-ip>:10001" -- è¿æ¥è¿œç¨‹é›†ç¾¤
headnode_ip = os.getenv("RAY_HEAD_IP")
multinode_depoly = MultiNodeDeployment(f"ray://{headnode_ip}:10001")
multinode_depoly.shut_down()

# å®šä¹‰ä»»åŠ¡ç±» 
class MyDeployment:     
    async def __call__(self, input, *args, **kargs): 
        if isinstance(input, Request):
            input = "get request"
        context = get_runtime_context() 
        node_ip = context.get_node_id()  # è·å–å½“å‰èŠ‚ç‚¹ IDï¼ˆRay åˆ†é…çš„å”¯ä¸€æ ‡è¯†ï¼‰ 
            
        # è·å–å½“å‰ä½¿ç”¨çš„ GPU IDï¼ˆå‡è®¾ä½¿ç”¨ PyTorchï¼‰ 
        if torch.cuda.is_available(): 
            gpu_id = torch.cuda.current_device() 
            gpu_name = torch.cuda.get_device_name(gpu_id) 
        else: 
            gpu_id = "CPU" 
            gpu_name = "No GPU available" 

        # æ‰“å°æ—¥å¿— 
        print(f"ğŸš€ğŸš€ğŸš€ æ¨ç†ä»»åŠ¡{input}è¿è¡Œåœ¨ï¼šNode {node_ip}ï¼ŒGPU {gpu_id} ({gpu_name})") 
        return f"node {node_ip} got {input}" 

# åˆå§‹åŒ–éƒ¨ç½²å¯¹è±¡   
multinode_depoly.initialize_deployment( 
    name='hello_world', 
    min_replicas=2,     # æœ€å°‘å‰¯æœ¬æ•° 
    max_replicas=2,     # æœ€å¤šå‰¯æœ¬æ•° 
    task_processor=MyDeployment,  # ç”¨æˆ·è‡ªå®šä¹‰ä»»åŠ¡API 
    num_gpus=1,
) 
    
# åœ¨é›†ç¾¤ç¯å¢ƒå¯åŠ¨éƒ¨ç½²å¯¹è±¡
multinode_depoly.run(port=8100) 


# ç­‰å¾…æœåŠ¡å¯åŠ¨ 
import time 
time.sleep(5) 


# å†æ¬¡æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
multinode_depoly.check_cluster_status()


# å•æ¬¡æ¨ç† 
print(multinode_depoly.inference("SINGLE TASK")) 
time.sleep(3)
    
# æ‰¹é‡æ¨ç† 
input_data = ['BATCH TASK' + str(i) for i in range(50)] 
asyncio.run(multinode_depoly.batch_forward(input_data))

# å…³é—­æœåŠ¡
# multinode_depoly.shut_down() 