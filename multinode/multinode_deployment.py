import requests 
import asyncio 
import ray 
from ray import serve
from ray.serve import Deployment 
from ray.runtime_env import RuntimeEnv 
from fastapi import FastAPI 
 

class MultiNodeDeployment:   
    def __init__(self, ray_address: str = None): 
        """ 
        å®Œæˆè¿æ¥ Ray å¹¶æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ 
        ray_address:  
            None[ç¼ºçœ] -- å•æœºå¯åŠ¨æœ¬åœ°rayé›†ç¾¤, ä»…æ­¤æ–¹å¼å¯åˆ›å»ºé›†ç¾¤ï¼Œå…¶ä»–å‡éœ€å…ˆå¯åŠ¨é›†ç¾¤ 
            "auto" -- é›†ç¾¤å†…è‡ªåŠ¨å‘ç°å¹¶è¿æ¥,è‡ªåŠ¨å‘ç°å¹¶è¿æ¥åˆ°åŒä¸€ç½‘ç»œå†…å·²å­˜åœ¨çš„Rayé›†ç¾¤,æ‰¾ä¸åˆ°æŠ›å‡ºé”™è¯¯ 
            "<head-node-ip>:<port>" - é›†ç¾¤å†…ç›´æ¥è¿æ¥,éœ€ä¼ å…¥å¤´èŠ‚ç‚¹ipå’Œport 
            "ray://<head-node-ip>:10001" -- é›†ç¾¤å¤–è¿œç¨‹è¿æ¥(Ray Client), 10001ä¸ºå¤´èŠ‚ç‚¹å¯åŠ¨Ray ClientæœåŠ¡é»˜è®¤ç›‘å¬ç«¯å£
        """ 
        self.cluster_config = {}
        self.deployment = None 
        self.deployment_handle = None
        self.deployment_name = None 
        self.head_node_ip = None
        self.url = None
        self.check_cluster_status(ray_address, print_status=True)
        # if ray_address and ":" in ray_address:
        #     self.head_node_ip = ray_address.split(':')[-2].split('//')[-1]
     
    def check_cluster_status(self, ray_address: str = None, print_status: bool = False): 
        """ 
        å¯åŠ¨ Ray å¹¶æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ 
 
        è¿”å›: 
            dict: åŒ…å«èŠ‚ç‚¹ä¿¡æ¯ã€æ€»èµ„æºæƒ…å†µä»¥åŠå¯ç”¨èµ„æºçš„å­—å…¸ã€‚ 
        """ 
        if not ray.is_initialized(): 
            if ray_address: 
                ray.init(address=ray_address) 
            else: 
                ray.init() 
                
        nodes = ray.nodes()
        resources = ray.cluster_resources()
        available_resources = ray.available_resources()
        
        self.cluster_config = { 
            "num_machines": len(nodes), 
            "num_gpus": resources.get('GPU', 0), 
            "num_cpus": resources.get('CPU', 0), 
            "available_gpus": available_resources.get('GPU', 0),
            "available_cpus": available_resources.get('CPU', 0), 
        }

        if not self.head_node_ip:
            for node in nodes:
                if node.get("IsHead", None) or node['Resources'].get("node:__internal_head__", None):
                    self.head_node_ip = node["NodeManagerAddress"].split(":")[0]
                    break
        if print_status:
            print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—") 
            print("â•‘        ğŸš€ é›†ç¾¤çŠ¶æ€         â•‘") 
            print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£") 
            print(f"â•‘ å¤´èŠ‚ç‚¹IP: {self.head_node_ip:<16} â•‘") 
            print(f"â•‘ èŠ‚ç‚¹æ€»æ•°: {len(nodes):<16} â•‘") 
            print(f"â•‘ CPU(æ€»): {resources.get('CPU', 0):<17} â•‘") 
            print(f"â•‘ CPU(å¯ç”¨): {available_resources.get('CPU', 0):<15} â•‘")
            print(f"â•‘ GPU(æ€»): {resources.get('GPU', 0):<17} â•‘") 
            print(f"â•‘ GPU(å¯ç”¨): {available_resources.get('GPU', 0):<15} â•‘")
            print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•") 
         
        return self.cluster_config | {"aplications": serve.status().applications}
    
    def initialize_deployment(self, name: str, 
                              task_processor,
                              min_replicas: int = 1,
                            max_replicas: int = 1, 
                           num_gpus: int = 0, num_cpus: int = 1,
                           runtime_env: RuntimeEnv=None, 
                           app: FastAPI = None) -> Deployment: 
        """ 
        åˆå§‹åŒ–éƒ¨ç½²ä»»åŠ¡å¯¹è±¡ 
 
        å‚æ•°: 
            name (str): éƒ¨ç½²åç§°ï¼Œæ ‡è¯†å”¯ä¸€çš„æœåŠ¡å®ä¾‹ã€‚ 
            min_replicas (int): æœ€å°å‰¯æœ¬æ•°ï¼Œæ§åˆ¶æœåŠ¡çš„æœ€ä½èµ„æºåˆ†é…ã€‚ 
            max_replicas (int): æœ€å¤§å‰¯æœ¬æ•°ï¼Œå†³å®šæœåŠ¡åœ¨è´Ÿè½½é«˜æ—¶å¯æ‰©å±•çš„æœ€å¤§å‰¯æœ¬æ•°ã€‚ 
            task_processor (Callable): ä»»åŠ¡å¤„ç†ç®¡é“ï¼Œå°è£…æ¨ç†æˆ–è®¡ç®—é€»è¾‘çš„å¯è°ƒç”¨å¯¹è±¡ã€‚ 
            num_gpus (int, å¯é€‰): æ¯ä¸ªå‰¯æœ¬æ‰€éœ€çš„ GPU æ•°é‡ï¼Œé»˜è®¤ä¸º 1ã€‚ 
        """ 
        self.deployment_name = name 
        # if serve.get_deployment_handle(name):
        #     assert f"deployment {name} already exists!"
        if min_replicas * num_gpus > self.cluster_config.get('num_gpus', 0): 
            assert "Insufficient GPU resources!!" 
        if min_replicas * num_cpus > self.cluster_config.get('num_cpus', 0): 
            assert "Insufficient CPU resources!!" 
 
        # å®šä¹‰ Ray Serve éƒ¨ç½²ç±»ï¼Œå†…éƒ¨å°è£… task_processor çš„è°ƒç”¨é€»è¾‘ 
        ray_actor_options = {"num_gpus": num_gpus} 
        if runtime_env: 
            ray_actor_options["runtime_env"] = runtime_env 
        if not app:
            serve_deployment = serve.deployment( 
                name=name, 
                ray_actor_options=ray_actor_options, 
                autoscaling_config={ 
                    "min_replicas": min_replicas, 
                    "max_replicas": max_replicas, 
                }, 
            )(task_processor) 
             
        else: 
            serve_deployment = serve.deployment( 
                name=name, 
                ray_actor_options=ray_actor_options, 
                autoscaling_config={ 
                    "min_replicas": min_replicas, 
                    "max_replicas": max_replicas, 
                }, 
            )(serve.ingress(app)(task_processor)) 
        
        # å°†éƒ¨ç½²ç»‘å®šä»»åŠ¡å¯¹è±¡ 
        self.deployment = serve_deployment.bind() 
    
    def connect_to_serve(self, name: str):
        self.deployment_handle = serve.get_deployment_handle(deployment_name=name, app_name=name)
        if not self.deployment_handle:
            assert f"deployment {name} not found!"
 
    def run(self, port: int = 8000, route_prefix: str = None): 
        """ 
        å¤šèŠ‚ç‚¹å¯åŠ¨å·²å®šä¹‰çš„éƒ¨ç½²ä»»åŠ¡ 
 
        å‚æ•°: 
            port (int): æœåŠ¡ç›‘å¬çš„ç«¯å£å·ã€‚ 
        """ 
        route_prefix = route_prefix or f"/{self.deployment_name}"
        if not route_prefix.startswith("/"):
            route_prefix = f"/{route_prefix}"
        
        if serve.context._global_client:
            current_port = serve.context._global_client._http_config.port
            if port != current_port:
                print(f"Serve is already running on port {current_port} !")
                port = current_port
                # raise RuntimeError(
                #     f"Serve is already running on port {current_port}. "
                #     "Please shut it down first to specify a different port."
                # )
            
        if not self.deployment: 
            raise ValueError("Empty deployment!") 
        serve.start(http_options={"port": port, "host": "0.0.0.0"}) 
        self.deployment_handle = serve.run(self.deployment, name=self.deployment_name, route_prefix=route_prefix) 
        self.url = f"http://{self.head_node_ip}:{port}{route_prefix}" 
        print(f"âœ… æœåŠ¡{self.deployment_name}å·²éƒ¨ç½²ï¼Œè®¿é—®åœ°å€ï¼š{self.url}")
     
    def inference(self, input_data: str = None): 
        """ 
        é€šè¿‡DeploymentHandleè¿›è¡ŒåŒæ­¥æ¨ç† 
        """ 
        if not self.deployment_handle:
            assert "Serve not found!"
        try: 
            # åŒæ­¥è°ƒç”¨ï¼ˆé€‚åˆå•æ¬¡è¯·æ±‚ï¼‰ 
            result = self.deployment_handle.remote(input_data).result() 
            print("æ¨ç†ç»“æœï¼š", result) 
            return result 
        except Exception as e: 
            print(f"è°ƒç”¨æœåŠ¡å¤±è´¥: {str(e)}") 
            return {"error": str(e)} 
 
    async def batch_forward(self, input_list): 
        """ 
        é€šè¿‡ DeploymentHandle è¿›è¡Œå¼‚æ­¥æ‰¹é‡æ¨ç† 
        """ 
        tasks = [self.deployment_handle.remote(data) for data in input_list]
        return await asyncio.gather(*tasks)
     
    def inference_url(self, input_data: str = None): 
        """ 
        é€šè¿‡ url è¿›è¡ŒåŒæ­¥æ¨ç† 
        """ 
        url = self.url 
        response = requests.post(url, json={"input": input_data}) 
        if response.status_code == 200: 
            print("æ¨ç†ç»“æœï¼š", response.json()) 
            return response.json() 
        else: 
            print("è°ƒç”¨æœåŠ¡å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š", response.status_code) 
     
    async def batch_forward_url(self, input_list, max_retries=3): 
        """ 
        é€šè¿‡ url è¿›è¡Œå¼‚æ­¥æ‰¹é‡æ¨ç† 
        """ 
        url = self.url 
        results = [] 
 
        async def request_with_retry(data, retries): 
            for attempt in range(retries): 
                response = requests.post(url, json={"input": data}) # éœ€æ”¹ä¸ºå¼‚æ­¥ 
                if response.status_code == 200: 
                    return response.json() 
                await asyncio.sleep(1) 
            return {"error": "è¯·æ±‚å¤±è´¥"} 
 
        tasks = [request_with_retry(data, max_retries) for data in input_list] 
        results = await asyncio.gather(*tasks) 
        return results 
 
    def shut_down(self, name: str = None): 
        if name:
            serve.delete(name)
        else:
            serve.shutdown()

 